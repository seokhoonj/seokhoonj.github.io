---
layout: post
title: 선형 회귀 모델
category: Math
tags: [Math, 선형회귀모델]
published: true
comments: false
---

선형 회귀 모델
---

## 회귀 모델의 목표
1. $D$차원의 벡터 $x$들이 입력(input)변수로 주어졌을 때, 그에 해당하는 연속 target 변수 t값을 예측하는 것
2. $N$개의 관측값 ${x\_n}$과 이에 해당하는 표적값 ${t\_n}$이 훈련 집합$(n = 1, \ldots, N)$으로 주어졌을 때 회귀 모델의 목표는 새 변수 $x$의 표적값 $t$를 예측하는 것
3. 확률적인 측면에서 말하자면, 예측 분포 $p(t\|x)$를 모델하는 것이 우리의 목표
4. 이 조건부 분포를 이용하면 어떤 새 $x$값에 대해서든 손실 함수의 기댓값을 최소화하는 표적값 $t$를 예측해 낼 수 있다.
5. 실숫값을 가지는 변수들에 대해 흔히 쓰이는 손실함수는 제곱 손실함수다.
6. 제곱 손실 함수를 사용할 경우 $t$에 대한 조건부 기댓값이 최적의 해가 된다.

## 선형 기저 함수 모델
1. 가장 단순한 형태의 선형회귀모델은 입력 변수들의 선형 결합을 바탕으로 한 모델이다.
\\( y(x, w) = w\_0 + w\_1x\_1 + ... + w\_Dx\_D  = \left(\begin{array}{ccc} w\_0 & w\_1 & \ldots & w\_D \end{array}\right) \left(\begin{array}{c} 1 \\\\ x\_1 \\\\ \vdots \\\\ x\_D \end{array}\right) = w^T x \\)
2. 선형 회귀 모델의 가장 중요한 성질은 바로 이 모델이 매개변수 $w\_0, \ldots, w\_D$의 선형함수라는 것이다. 이 한계점을 극복하기 위해서 다음처럼 입력 변수에 대한 고정 비선형 함수들의 선형 결합을 사용할 수 있다.
\\( y(x, w) = w\_0 + \sum\_{j=1}^{M-1} w\_j\phi\_j(x) \\)
 - 여기서 $\phi\_j(x)$가 기저 함수(basis function)다. 인덱스 $j$의 최대값이 $M-1$이므로 이 모델의 매개변수의 총 숫자는 $M$이다.
 - 편의를 위해서 추가적인 의사 '기저 함수' $\phi\_0(x) = 1$을 정의하도록 하자. 그러면 다음과 같이 표현할 수 있다.
 \\( y(x, w) = \sum\_{j=1}^{M-1} w\_j\phi\_j(x) = w^T \phi(x) \\)
 - 많은 패턴 인식의 응용 사례에서는 원 데이터 변수에 <em><strong>전처리</strong></em>나 <em><strong>특징 추출 과정</strong></em>을 적용하게 된다. 만약 원래의 변수가 벡터 $x$라 한다면, 특징들은 기저 함수 ${\phi\_j(x)}$를 바탕으로 표현할 수 있다.
 - 비선형 기저 함수들을 사용하여 함수 $y(x,w)$가 입력 벡터 $x$에 대한 비선형 함수가 되도록 할 수 있다. 그럼에도 불구하고 이런 형태를 가진 함수들이 선형 모델이라고 불리는 이유는 이 함수들이 $w$에 대해서 선형 함수이기 때문이다.
 - 다양한 다른 함수들이 기저 함수로 사용될 수 있다. 예를 들어, 다음이 그 중 하나다.
 \\( \phi\_j(x) = exp\left\lbrace-\frac{(x-\mu\_j)^2}{2s^2}\right\rbrace  = exp\left\lbrace-\frac{1}{2}\left(\frac{x-\mu\_j}{s}\right)^2\right\rbrace = e^{-\frac{1}{2}z^2} \\) 
